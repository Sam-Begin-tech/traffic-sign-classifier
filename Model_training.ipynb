{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17110e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in .\\newenv\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\newenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: six>=1.5 in .\\newenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'd:\\python\\traffic_signal\\newenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e747a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5822141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Dataset\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.root_dir + '/' + self.data.iloc[idx]['Path']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = int(self.data.iloc[idx]['ClassId'])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de29e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Load train and test datasets\n",
    "train_dataset = TrafficDataset(csv_file='archive/Train.csv', root_dir='archive', transform=transform)\n",
    "valid_dataset = TrafficDataset(csv_file='archive/Test.csv', root_dir='archive', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf80b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\traffic_signal\\newenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\python\\traffic_signal\\newenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 5. Load Pretrained Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_classes = len(set(train_dataset.data['ClassId']))\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# 6. Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3194d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 100%|██████████| 1226/1226 [09:26<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1907, Accuracy: 95.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 100%|██████████| 1226/1226 [07:53<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0089, Accuracy: 99.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 100%|██████████| 1226/1226 [06:08<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.0077, Accuracy: 99.82%\n",
      "Validation Accuracy: 99.14%\n"
     ]
    }
   ],
   "source": [
    "# 7. Training loop\n",
    "num_epochs=3\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%\")\n",
    "\n",
    "# 8. Validation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {100*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbe05e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 34\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_image(image_path):\n",
    "    # Define the same transformations used during training\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),        # Resize to match input size\n",
    "        transforms.ToTensor(),                # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open and convert the image\n",
    "    image = transform(image).unsqueeze(0)          # Add batch dimension (1,)\n",
    "    return image\n",
    "\n",
    "# Load a sample image\n",
    "image_path = \"left.png\"  # Replace with your image path\n",
    "image = load_image(image_path)\n",
    "\n",
    "# Send the image to the same device as the model (GPU or CPU)\n",
    "image = image.to(device)\n",
    "\n",
    "# Make the prediction\n",
    "with torch.no_grad():  # Turn off gradients for inference\n",
    "    output = model(image)\n",
    "    \n",
    "# Get the predicted class\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "\n",
    "# Display the predicted class\n",
    "print(f\"Predicted Class: {predicted_class.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c55b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'traffic_sign_model.pth')\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd5b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: \"Speed limit (20km/h)\",\n",
    "    1: \"Speed limit (30km/h)\",\n",
    "    2: \"Speed limit (50km/h)\",\n",
    "    3: \"Speed limit (60km/h)\",\n",
    "    4: \"Speed limit (70km/h)\",\n",
    "    5: \"Speed limit (80km/h)\",\n",
    "    6: \"End of speed limit (80km/h)\",\n",
    "    7: \"Speed limit (100km/h)\",\n",
    "    8: \"Speed limit (120km/h)\",\n",
    "    9: \"No passing\",\n",
    "    10: \"No passing for vehicles over 3.5 metric tons\",\n",
    "    11: \"Right-of-way at the next intersection\",\n",
    "    12: \"Priority road\",\n",
    "    13: \"Yield\",\n",
    "    14: \"Stop\",\n",
    "    15: \"No vehicles\",\n",
    "    16: \"Vehicles over 3.5 metric tons prohibited\",\n",
    "    17: \"No entry\",\n",
    "    18: \"General caution\",\n",
    "    19: \"Dangerous curve to the left\",\n",
    "    20: \"Dangerous curve to the right\",\n",
    "    21: \"Double curve\",\n",
    "    22: \"Bumpy road\",\n",
    "    23: \"Slippery road\",\n",
    "    24: \"Road narrows on the right\",\n",
    "    25: \"Road work\",\n",
    "    26: \"Traffic signals\",\n",
    "    27: \"Pedestrians\",\n",
    "    28: \"Children crossing\",\n",
    "    29: \"Bicycles crossing\",\n",
    "    30: \"Beware of ice/snow\",\n",
    "    31: \"Wild animals crossing\",\n",
    "    32: \"End of all speed and passing limits\",\n",
    "    33: \"Turn right ahead\",\n",
    "    34: \"Turn left ahead\",\n",
    "    35: \"Ahead only\",\n",
    "    36: \"Go straight or right\",\n",
    "    37: \"Go straight or left\",\n",
    "    38: \"Keep right\",\n",
    "    39: \"Keep left\",\n",
    "    40: \"Roundabout mandatory\",\n",
    "    41: \"End of no passing\",\n",
    "    42: \"End of no passing by vehicles over 3.5 metric tons\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40240a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\traffic_signal\\newenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\python\\traffic_signal\\newenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22880\\1182255130.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth', map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Predicted Class: 14\n",
      "Predicted Class: Stop\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Define the model architecture (same as before)\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 43  # replace 43 with your actual number of classes\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# 2. Load the saved weights\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth', map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "# 3. Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e571b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 22\n",
      "Predicted Class: Bumpy road\n"
     ]
    }
   ],
   "source": [
    "# 4. Load and Predict an image\n",
    "def predict(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Example usage\n",
    "image_path = 'br.webp'  # replace this\n",
    "predicted_label = predict(image_path)\n",
    "print(f\"Predicted Class: {predicted_label}\")\n",
    "\n",
    "\n",
    "\n",
    "class_name = class_names.get(predicted_label, \"Unknown Class\")\n",
    "print(f\"Predicted Class: {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b91bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
